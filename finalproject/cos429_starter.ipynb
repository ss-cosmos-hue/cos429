{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import zipfile\n",
        "# with zipfile.ZipFile(\"dataset-resized.zip\", 'r') as zip_ref:\n",
        "#     zip_ref.extractall(\"trashnet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "f3_3Xk8_SW_c"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import resnet34,ResNet34_Weights\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torchsummary import summary\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mhdPpxn-SeRf"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_img = Image.open(\"trashnet/dataset-resized/plastic/plastic1.jpg\")\n",
        "sample_img_arr = np.array(sample_img)\n",
        "H,W,_ = np.shape(sample_img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "classes = [\"cardboard\",\"glass\",\"metal\",\"paper\",\"plastic\",\"trash\"]\n",
        "class_sizes = {\"cardboard\":403,\"glass\":501,\"metal\":410,\"paper\":594,\"plastic\":482,\"trash\":137}\n",
        "class_nums =  {\"cardboard\":0,\"glass\":1,\"metal\":2,\"paper\":3,\"plastic\":4,\"trash\":5}\n",
        "\n",
        "n_whole = np.sum(list(class_sizes.values()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = np.zeros((n_whole,H,W,3))\n",
        "y = np.zeros((n_whole,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2527"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n_whole"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total_count = 0\n",
            "total_count = 300\n",
            "total_count = 600\n",
            "total_count = 900\n",
            "total_count = 1200\n",
            "total_count = 1500\n",
            "total_count = 1800\n",
            "total_count = 2100\n",
            "total_count = 2400\n"
          ]
        }
      ],
      "source": [
        "#update X and y\n",
        "total_count  = 0\n",
        "for categ in classes:\n",
        "    n_elem = class_sizes[categ]\n",
        "    for i in range(1,1+n_elem):\n",
        "        if total_count%300 == 0:\n",
        "            print(f'total_count = {total_count}')\n",
        "        X[total_count] = np.array(Image.open(f'trashnet/dataset-resized/{categ}/{categ}{i}.jpg'))\n",
        "        y[total_count] = class_nums[categ]\n",
        "        total_count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = X.reshape((n_whole,3,H,W))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_X,test_X,train_y,test_y = train_test_split(X, y, test_size=500, random_state=42)\n",
        "val_X,test_X , val_y,test_y = train_test_split(test_X, test_y, test_size=250, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train_X = torch.LongTensor(train_X)\n",
        "# train_y = torch.LongTensor(train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_X = torch.LongTensor(train_X)\n",
        "train_y = torch.LongTensor(train_y)\n",
        "train_dataset = torch.utils.data.TensorDataset(train_X, train_y)\n",
        "trainloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "#not sure we would uuse validationloader or testloader\n",
        "val_X = torch.LongTensor(val_X)\n",
        "val_y = torch.LongTensor(val_y)\n",
        "val_dataset = torch.utils.data.TensorDataset(val_X, val_y)\n",
        "valloader = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
        "\n",
        "test_X = torch.LongTensor(test_X)\n",
        "test_y = torch.LongTensor(test_y)\n",
        "test_dataset = torch.utils.data.TensorDataset(test_X, test_y)\n",
        "testloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.dtype"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(train_X.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataloaders = {'train':trainloader,'val':valloader,'test':testloader}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([2027, 3, 384, 512]),\n",
              " torch.Size([250, 3, 384, 512]),\n",
              " torch.Size([250, 3, 384, 512]))"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.shape(train_X),np.shape(val_X),np.shape(test_X)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## model initialization "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "DmipUJJPTIhy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " first layer: Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n"
          ]
        }
      ],
      "source": [
        "model = resnet34(weights = ResNet34_Weights.DEFAULT)\n",
        "first_layer = list(model.children())[0]\n",
        "print(f' first layer: {first_layer}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), \"model_weights.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "wFHrGk7c4nXR",
        "outputId": "01dd6d68-3090-400f-afb9-16973cc0ae3d"
      },
      "outputs": [],
      "source": [
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "dcMkamhY4aTO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "except last layer: Linear(in_features=512, out_features=6, bias=True)\n"
          ]
        }
      ],
      "source": [
        "# freeze all layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "last_layer = list(model.children())[-1]\n",
        "print(f'except last layer: {last_layer}')\n",
        "for param in last_layer.parameters():\n",
        "    param.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "├─Conv2d: 1-1                            [-1, 64, 192, 256]        (9,408)\n",
            "├─BatchNorm2d: 1-2                       [-1, 64, 192, 256]        (128)\n",
            "├─ReLU: 1-3                              [-1, 64, 192, 256]        --\n",
            "├─MaxPool2d: 1-4                         [-1, 64, 96, 128]         --\n",
            "├─Sequential: 1-5                        [-1, 64, 96, 128]         --\n",
            "|    └─BasicBlock: 2-1                   [-1, 64, 96, 128]         --\n",
            "|    |    └─Conv2d: 3-1                  [-1, 64, 96, 128]         (36,864)\n",
            "|    |    └─BatchNorm2d: 3-2             [-1, 64, 96, 128]         (128)\n",
            "|    |    └─ReLU: 3-3                    [-1, 64, 96, 128]         --\n",
            "|    |    └─Conv2d: 3-4                  [-1, 64, 96, 128]         (36,864)\n",
            "|    |    └─BatchNorm2d: 3-5             [-1, 64, 96, 128]         (128)\n",
            "|    |    └─ReLU: 3-6                    [-1, 64, 96, 128]         --\n",
            "|    └─BasicBlock: 2-2                   [-1, 64, 96, 128]         --\n",
            "|    |    └─Conv2d: 3-7                  [-1, 64, 96, 128]         (36,864)\n",
            "|    |    └─BatchNorm2d: 3-8             [-1, 64, 96, 128]         (128)\n",
            "|    |    └─ReLU: 3-9                    [-1, 64, 96, 128]         --\n",
            "|    |    └─Conv2d: 3-10                 [-1, 64, 96, 128]         (36,864)\n",
            "|    |    └─BatchNorm2d: 3-11            [-1, 64, 96, 128]         (128)\n",
            "|    |    └─ReLU: 3-12                   [-1, 64, 96, 128]         --\n",
            "|    └─BasicBlock: 2-3                   [-1, 64, 96, 128]         --\n",
            "|    |    └─Conv2d: 3-13                 [-1, 64, 96, 128]         (36,864)\n",
            "|    |    └─BatchNorm2d: 3-14            [-1, 64, 96, 128]         (128)\n",
            "|    |    └─ReLU: 3-15                   [-1, 64, 96, 128]         --\n",
            "|    |    └─Conv2d: 3-16                 [-1, 64, 96, 128]         (36,864)\n",
            "|    |    └─BatchNorm2d: 3-17            [-1, 64, 96, 128]         (128)\n",
            "|    |    └─ReLU: 3-18                   [-1, 64, 96, 128]         --\n",
            "├─Sequential: 1-6                        [-1, 128, 48, 64]         --\n",
            "|    └─BasicBlock: 2-4                   [-1, 128, 48, 64]         --\n",
            "|    |    └─Conv2d: 3-19                 [-1, 128, 48, 64]         (73,728)\n",
            "|    |    └─BatchNorm2d: 3-20            [-1, 128, 48, 64]         (256)\n",
            "|    |    └─ReLU: 3-21                   [-1, 128, 48, 64]         --\n",
            "|    |    └─Conv2d: 3-22                 [-1, 128, 48, 64]         (147,456)\n",
            "|    |    └─BatchNorm2d: 3-23            [-1, 128, 48, 64]         (256)\n",
            "|    |    └─Sequential: 3-24             [-1, 128, 48, 64]         (8,448)\n",
            "|    |    └─ReLU: 3-25                   [-1, 128, 48, 64]         --\n",
            "|    └─BasicBlock: 2-5                   [-1, 128, 48, 64]         --\n",
            "|    |    └─Conv2d: 3-26                 [-1, 128, 48, 64]         (147,456)\n",
            "|    |    └─BatchNorm2d: 3-27            [-1, 128, 48, 64]         (256)\n",
            "|    |    └─ReLU: 3-28                   [-1, 128, 48, 64]         --\n",
            "|    |    └─Conv2d: 3-29                 [-1, 128, 48, 64]         (147,456)\n",
            "|    |    └─BatchNorm2d: 3-30            [-1, 128, 48, 64]         (256)\n",
            "|    |    └─ReLU: 3-31                   [-1, 128, 48, 64]         --\n",
            "|    └─BasicBlock: 2-6                   [-1, 128, 48, 64]         --\n",
            "|    |    └─Conv2d: 3-32                 [-1, 128, 48, 64]         (147,456)\n",
            "|    |    └─BatchNorm2d: 3-33            [-1, 128, 48, 64]         (256)\n",
            "|    |    └─ReLU: 3-34                   [-1, 128, 48, 64]         --\n",
            "|    |    └─Conv2d: 3-35                 [-1, 128, 48, 64]         (147,456)\n",
            "|    |    └─BatchNorm2d: 3-36            [-1, 128, 48, 64]         (256)\n",
            "|    |    └─ReLU: 3-37                   [-1, 128, 48, 64]         --\n",
            "|    └─BasicBlock: 2-7                   [-1, 128, 48, 64]         --\n",
            "|    |    └─Conv2d: 3-38                 [-1, 128, 48, 64]         (147,456)\n",
            "|    |    └─BatchNorm2d: 3-39            [-1, 128, 48, 64]         (256)\n",
            "|    |    └─ReLU: 3-40                   [-1, 128, 48, 64]         --\n",
            "|    |    └─Conv2d: 3-41                 [-1, 128, 48, 64]         (147,456)\n",
            "|    |    └─BatchNorm2d: 3-42            [-1, 128, 48, 64]         (256)\n",
            "|    |    └─ReLU: 3-43                   [-1, 128, 48, 64]         --\n",
            "├─Sequential: 1-7                        [-1, 256, 24, 32]         --\n",
            "|    └─BasicBlock: 2-8                   [-1, 256, 24, 32]         --\n",
            "|    |    └─Conv2d: 3-44                 [-1, 256, 24, 32]         (294,912)\n",
            "|    |    └─BatchNorm2d: 3-45            [-1, 256, 24, 32]         (512)\n",
            "|    |    └─ReLU: 3-46                   [-1, 256, 24, 32]         --\n",
            "|    |    └─Conv2d: 3-47                 [-1, 256, 24, 32]         (589,824)\n",
            "|    |    └─BatchNorm2d: 3-48            [-1, 256, 24, 32]         (512)\n",
            "|    |    └─Sequential: 3-49             [-1, 256, 24, 32]         (33,280)\n",
            "|    |    └─ReLU: 3-50                   [-1, 256, 24, 32]         --\n",
            "|    └─BasicBlock: 2-9                   [-1, 256, 24, 32]         --\n",
            "|    |    └─Conv2d: 3-51                 [-1, 256, 24, 32]         (589,824)\n",
            "|    |    └─BatchNorm2d: 3-52            [-1, 256, 24, 32]         (512)\n",
            "|    |    └─ReLU: 3-53                   [-1, 256, 24, 32]         --\n",
            "|    |    └─Conv2d: 3-54                 [-1, 256, 24, 32]         (589,824)\n",
            "|    |    └─BatchNorm2d: 3-55            [-1, 256, 24, 32]         (512)\n",
            "|    |    └─ReLU: 3-56                   [-1, 256, 24, 32]         --\n",
            "|    └─BasicBlock: 2-10                  [-1, 256, 24, 32]         --\n",
            "|    |    └─Conv2d: 3-57                 [-1, 256, 24, 32]         (589,824)\n",
            "|    |    └─BatchNorm2d: 3-58            [-1, 256, 24, 32]         (512)\n",
            "|    |    └─ReLU: 3-59                   [-1, 256, 24, 32]         --\n",
            "|    |    └─Conv2d: 3-60                 [-1, 256, 24, 32]         (589,824)\n",
            "|    |    └─BatchNorm2d: 3-61            [-1, 256, 24, 32]         (512)\n",
            "|    |    └─ReLU: 3-62                   [-1, 256, 24, 32]         --\n",
            "|    └─BasicBlock: 2-11                  [-1, 256, 24, 32]         --\n",
            "|    |    └─Conv2d: 3-63                 [-1, 256, 24, 32]         (589,824)\n",
            "|    |    └─BatchNorm2d: 3-64            [-1, 256, 24, 32]         (512)\n",
            "|    |    └─ReLU: 3-65                   [-1, 256, 24, 32]         --\n",
            "|    |    └─Conv2d: 3-66                 [-1, 256, 24, 32]         (589,824)\n",
            "|    |    └─BatchNorm2d: 3-67            [-1, 256, 24, 32]         (512)\n",
            "|    |    └─ReLU: 3-68                   [-1, 256, 24, 32]         --\n",
            "|    └─BasicBlock: 2-12                  [-1, 256, 24, 32]         --\n",
            "|    |    └─Conv2d: 3-69                 [-1, 256, 24, 32]         (589,824)\n",
            "|    |    └─BatchNorm2d: 3-70            [-1, 256, 24, 32]         (512)\n",
            "|    |    └─ReLU: 3-71                   [-1, 256, 24, 32]         --\n",
            "|    |    └─Conv2d: 3-72                 [-1, 256, 24, 32]         (589,824)\n",
            "|    |    └─BatchNorm2d: 3-73            [-1, 256, 24, 32]         (512)\n",
            "|    |    └─ReLU: 3-74                   [-1, 256, 24, 32]         --\n",
            "|    └─BasicBlock: 2-13                  [-1, 256, 24, 32]         --\n",
            "|    |    └─Conv2d: 3-75                 [-1, 256, 24, 32]         (589,824)\n",
            "|    |    └─BatchNorm2d: 3-76            [-1, 256, 24, 32]         (512)\n",
            "|    |    └─ReLU: 3-77                   [-1, 256, 24, 32]         --\n",
            "|    |    └─Conv2d: 3-78                 [-1, 256, 24, 32]         (589,824)\n",
            "|    |    └─BatchNorm2d: 3-79            [-1, 256, 24, 32]         (512)\n",
            "|    |    └─ReLU: 3-80                   [-1, 256, 24, 32]         --\n",
            "├─Sequential: 1-8                        [-1, 512, 12, 16]         --\n",
            "|    └─BasicBlock: 2-14                  [-1, 512, 12, 16]         --\n",
            "|    |    └─Conv2d: 3-81                 [-1, 512, 12, 16]         (1,179,648)\n",
            "|    |    └─BatchNorm2d: 3-82            [-1, 512, 12, 16]         (1,024)\n",
            "|    |    └─ReLU: 3-83                   [-1, 512, 12, 16]         --\n",
            "|    |    └─Conv2d: 3-84                 [-1, 512, 12, 16]         (2,359,296)\n",
            "|    |    └─BatchNorm2d: 3-85            [-1, 512, 12, 16]         (1,024)\n",
            "|    |    └─Sequential: 3-86             [-1, 512, 12, 16]         (132,096)\n",
            "|    |    └─ReLU: 3-87                   [-1, 512, 12, 16]         --\n",
            "|    └─BasicBlock: 2-15                  [-1, 512, 12, 16]         --\n",
            "|    |    └─Conv2d: 3-88                 [-1, 512, 12, 16]         (2,359,296)\n",
            "|    |    └─BatchNorm2d: 3-89            [-1, 512, 12, 16]         (1,024)\n",
            "|    |    └─ReLU: 3-90                   [-1, 512, 12, 16]         --\n",
            "|    |    └─Conv2d: 3-91                 [-1, 512, 12, 16]         (2,359,296)\n",
            "|    |    └─BatchNorm2d: 3-92            [-1, 512, 12, 16]         (1,024)\n",
            "|    |    └─ReLU: 3-93                   [-1, 512, 12, 16]         --\n",
            "|    └─BasicBlock: 2-16                  [-1, 512, 12, 16]         --\n",
            "|    |    └─Conv2d: 3-94                 [-1, 512, 12, 16]         (2,359,296)\n",
            "|    |    └─BatchNorm2d: 3-95            [-1, 512, 12, 16]         (1,024)\n",
            "|    |    └─ReLU: 3-96                   [-1, 512, 12, 16]         --\n",
            "|    |    └─Conv2d: 3-97                 [-1, 512, 12, 16]         (2,359,296)\n",
            "|    |    └─BatchNorm2d: 3-98            [-1, 512, 12, 16]         (1,024)\n",
            "|    |    └─ReLU: 3-99                   [-1, 512, 12, 16]         --\n",
            "├─AdaptiveAvgPool2d: 1-9                 [-1, 512, 1, 1]           --\n",
            "├─Linear: 1-10                           [-1, 6]                   3,078\n",
            "==========================================================================================\n",
            "Total params: 21,287,750\n",
            "Trainable params: 3,078\n",
            "Non-trainable params: 21,284,672\n",
            "Total mult-adds (G): 14.40\n",
            "==========================================================================================\n",
            "Input size (MB): 2.25\n",
            "Forward/backward pass size (MB): 223.50\n",
            "Params size (MB): 81.21\n",
            "Estimated Total Size (MB): 306.96\n",
            "==========================================================================================\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "├─Conv2d: 1-1                            [-1, 64, 192, 256]        (9,408)\n",
              "├─BatchNorm2d: 1-2                       [-1, 64, 192, 256]        (128)\n",
              "├─ReLU: 1-3                              [-1, 64, 192, 256]        --\n",
              "├─MaxPool2d: 1-4                         [-1, 64, 96, 128]         --\n",
              "├─Sequential: 1-5                        [-1, 64, 96, 128]         --\n",
              "|    └─BasicBlock: 2-1                   [-1, 64, 96, 128]         --\n",
              "|    |    └─Conv2d: 3-1                  [-1, 64, 96, 128]         (36,864)\n",
              "|    |    └─BatchNorm2d: 3-2             [-1, 64, 96, 128]         (128)\n",
              "|    |    └─ReLU: 3-3                    [-1, 64, 96, 128]         --\n",
              "|    |    └─Conv2d: 3-4                  [-1, 64, 96, 128]         (36,864)\n",
              "|    |    └─BatchNorm2d: 3-5             [-1, 64, 96, 128]         (128)\n",
              "|    |    └─ReLU: 3-6                    [-1, 64, 96, 128]         --\n",
              "|    └─BasicBlock: 2-2                   [-1, 64, 96, 128]         --\n",
              "|    |    └─Conv2d: 3-7                  [-1, 64, 96, 128]         (36,864)\n",
              "|    |    └─BatchNorm2d: 3-8             [-1, 64, 96, 128]         (128)\n",
              "|    |    └─ReLU: 3-9                    [-1, 64, 96, 128]         --\n",
              "|    |    └─Conv2d: 3-10                 [-1, 64, 96, 128]         (36,864)\n",
              "|    |    └─BatchNorm2d: 3-11            [-1, 64, 96, 128]         (128)\n",
              "|    |    └─ReLU: 3-12                   [-1, 64, 96, 128]         --\n",
              "|    └─BasicBlock: 2-3                   [-1, 64, 96, 128]         --\n",
              "|    |    └─Conv2d: 3-13                 [-1, 64, 96, 128]         (36,864)\n",
              "|    |    └─BatchNorm2d: 3-14            [-1, 64, 96, 128]         (128)\n",
              "|    |    └─ReLU: 3-15                   [-1, 64, 96, 128]         --\n",
              "|    |    └─Conv2d: 3-16                 [-1, 64, 96, 128]         (36,864)\n",
              "|    |    └─BatchNorm2d: 3-17            [-1, 64, 96, 128]         (128)\n",
              "|    |    └─ReLU: 3-18                   [-1, 64, 96, 128]         --\n",
              "├─Sequential: 1-6                        [-1, 128, 48, 64]         --\n",
              "|    └─BasicBlock: 2-4                   [-1, 128, 48, 64]         --\n",
              "|    |    └─Conv2d: 3-19                 [-1, 128, 48, 64]         (73,728)\n",
              "|    |    └─BatchNorm2d: 3-20            [-1, 128, 48, 64]         (256)\n",
              "|    |    └─ReLU: 3-21                   [-1, 128, 48, 64]         --\n",
              "|    |    └─Conv2d: 3-22                 [-1, 128, 48, 64]         (147,456)\n",
              "|    |    └─BatchNorm2d: 3-23            [-1, 128, 48, 64]         (256)\n",
              "|    |    └─Sequential: 3-24             [-1, 128, 48, 64]         (8,448)\n",
              "|    |    └─ReLU: 3-25                   [-1, 128, 48, 64]         --\n",
              "|    └─BasicBlock: 2-5                   [-1, 128, 48, 64]         --\n",
              "|    |    └─Conv2d: 3-26                 [-1, 128, 48, 64]         (147,456)\n",
              "|    |    └─BatchNorm2d: 3-27            [-1, 128, 48, 64]         (256)\n",
              "|    |    └─ReLU: 3-28                   [-1, 128, 48, 64]         --\n",
              "|    |    └─Conv2d: 3-29                 [-1, 128, 48, 64]         (147,456)\n",
              "|    |    └─BatchNorm2d: 3-30            [-1, 128, 48, 64]         (256)\n",
              "|    |    └─ReLU: 3-31                   [-1, 128, 48, 64]         --\n",
              "|    └─BasicBlock: 2-6                   [-1, 128, 48, 64]         --\n",
              "|    |    └─Conv2d: 3-32                 [-1, 128, 48, 64]         (147,456)\n",
              "|    |    └─BatchNorm2d: 3-33            [-1, 128, 48, 64]         (256)\n",
              "|    |    └─ReLU: 3-34                   [-1, 128, 48, 64]         --\n",
              "|    |    └─Conv2d: 3-35                 [-1, 128, 48, 64]         (147,456)\n",
              "|    |    └─BatchNorm2d: 3-36            [-1, 128, 48, 64]         (256)\n",
              "|    |    └─ReLU: 3-37                   [-1, 128, 48, 64]         --\n",
              "|    └─BasicBlock: 2-7                   [-1, 128, 48, 64]         --\n",
              "|    |    └─Conv2d: 3-38                 [-1, 128, 48, 64]         (147,456)\n",
              "|    |    └─BatchNorm2d: 3-39            [-1, 128, 48, 64]         (256)\n",
              "|    |    └─ReLU: 3-40                   [-1, 128, 48, 64]         --\n",
              "|    |    └─Conv2d: 3-41                 [-1, 128, 48, 64]         (147,456)\n",
              "|    |    └─BatchNorm2d: 3-42            [-1, 128, 48, 64]         (256)\n",
              "|    |    └─ReLU: 3-43                   [-1, 128, 48, 64]         --\n",
              "├─Sequential: 1-7                        [-1, 256, 24, 32]         --\n",
              "|    └─BasicBlock: 2-8                   [-1, 256, 24, 32]         --\n",
              "|    |    └─Conv2d: 3-44                 [-1, 256, 24, 32]         (294,912)\n",
              "|    |    └─BatchNorm2d: 3-45            [-1, 256, 24, 32]         (512)\n",
              "|    |    └─ReLU: 3-46                   [-1, 256, 24, 32]         --\n",
              "|    |    └─Conv2d: 3-47                 [-1, 256, 24, 32]         (589,824)\n",
              "|    |    └─BatchNorm2d: 3-48            [-1, 256, 24, 32]         (512)\n",
              "|    |    └─Sequential: 3-49             [-1, 256, 24, 32]         (33,280)\n",
              "|    |    └─ReLU: 3-50                   [-1, 256, 24, 32]         --\n",
              "|    └─BasicBlock: 2-9                   [-1, 256, 24, 32]         --\n",
              "|    |    └─Conv2d: 3-51                 [-1, 256, 24, 32]         (589,824)\n",
              "|    |    └─BatchNorm2d: 3-52            [-1, 256, 24, 32]         (512)\n",
              "|    |    └─ReLU: 3-53                   [-1, 256, 24, 32]         --\n",
              "|    |    └─Conv2d: 3-54                 [-1, 256, 24, 32]         (589,824)\n",
              "|    |    └─BatchNorm2d: 3-55            [-1, 256, 24, 32]         (512)\n",
              "|    |    └─ReLU: 3-56                   [-1, 256, 24, 32]         --\n",
              "|    └─BasicBlock: 2-10                  [-1, 256, 24, 32]         --\n",
              "|    |    └─Conv2d: 3-57                 [-1, 256, 24, 32]         (589,824)\n",
              "|    |    └─BatchNorm2d: 3-58            [-1, 256, 24, 32]         (512)\n",
              "|    |    └─ReLU: 3-59                   [-1, 256, 24, 32]         --\n",
              "|    |    └─Conv2d: 3-60                 [-1, 256, 24, 32]         (589,824)\n",
              "|    |    └─BatchNorm2d: 3-61            [-1, 256, 24, 32]         (512)\n",
              "|    |    └─ReLU: 3-62                   [-1, 256, 24, 32]         --\n",
              "|    └─BasicBlock: 2-11                  [-1, 256, 24, 32]         --\n",
              "|    |    └─Conv2d: 3-63                 [-1, 256, 24, 32]         (589,824)\n",
              "|    |    └─BatchNorm2d: 3-64            [-1, 256, 24, 32]         (512)\n",
              "|    |    └─ReLU: 3-65                   [-1, 256, 24, 32]         --\n",
              "|    |    └─Conv2d: 3-66                 [-1, 256, 24, 32]         (589,824)\n",
              "|    |    └─BatchNorm2d: 3-67            [-1, 256, 24, 32]         (512)\n",
              "|    |    └─ReLU: 3-68                   [-1, 256, 24, 32]         --\n",
              "|    └─BasicBlock: 2-12                  [-1, 256, 24, 32]         --\n",
              "|    |    └─Conv2d: 3-69                 [-1, 256, 24, 32]         (589,824)\n",
              "|    |    └─BatchNorm2d: 3-70            [-1, 256, 24, 32]         (512)\n",
              "|    |    └─ReLU: 3-71                   [-1, 256, 24, 32]         --\n",
              "|    |    └─Conv2d: 3-72                 [-1, 256, 24, 32]         (589,824)\n",
              "|    |    └─BatchNorm2d: 3-73            [-1, 256, 24, 32]         (512)\n",
              "|    |    └─ReLU: 3-74                   [-1, 256, 24, 32]         --\n",
              "|    └─BasicBlock: 2-13                  [-1, 256, 24, 32]         --\n",
              "|    |    └─Conv2d: 3-75                 [-1, 256, 24, 32]         (589,824)\n",
              "|    |    └─BatchNorm2d: 3-76            [-1, 256, 24, 32]         (512)\n",
              "|    |    └─ReLU: 3-77                   [-1, 256, 24, 32]         --\n",
              "|    |    └─Conv2d: 3-78                 [-1, 256, 24, 32]         (589,824)\n",
              "|    |    └─BatchNorm2d: 3-79            [-1, 256, 24, 32]         (512)\n",
              "|    |    └─ReLU: 3-80                   [-1, 256, 24, 32]         --\n",
              "├─Sequential: 1-8                        [-1, 512, 12, 16]         --\n",
              "|    └─BasicBlock: 2-14                  [-1, 512, 12, 16]         --\n",
              "|    |    └─Conv2d: 3-81                 [-1, 512, 12, 16]         (1,179,648)\n",
              "|    |    └─BatchNorm2d: 3-82            [-1, 512, 12, 16]         (1,024)\n",
              "|    |    └─ReLU: 3-83                   [-1, 512, 12, 16]         --\n",
              "|    |    └─Conv2d: 3-84                 [-1, 512, 12, 16]         (2,359,296)\n",
              "|    |    └─BatchNorm2d: 3-85            [-1, 512, 12, 16]         (1,024)\n",
              "|    |    └─Sequential: 3-86             [-1, 512, 12, 16]         (132,096)\n",
              "|    |    └─ReLU: 3-87                   [-1, 512, 12, 16]         --\n",
              "|    └─BasicBlock: 2-15                  [-1, 512, 12, 16]         --\n",
              "|    |    └─Conv2d: 3-88                 [-1, 512, 12, 16]         (2,359,296)\n",
              "|    |    └─BatchNorm2d: 3-89            [-1, 512, 12, 16]         (1,024)\n",
              "|    |    └─ReLU: 3-90                   [-1, 512, 12, 16]         --\n",
              "|    |    └─Conv2d: 3-91                 [-1, 512, 12, 16]         (2,359,296)\n",
              "|    |    └─BatchNorm2d: 3-92            [-1, 512, 12, 16]         (1,024)\n",
              "|    |    └─ReLU: 3-93                   [-1, 512, 12, 16]         --\n",
              "|    └─BasicBlock: 2-16                  [-1, 512, 12, 16]         --\n",
              "|    |    └─Conv2d: 3-94                 [-1, 512, 12, 16]         (2,359,296)\n",
              "|    |    └─BatchNorm2d: 3-95            [-1, 512, 12, 16]         (1,024)\n",
              "|    |    └─ReLU: 3-96                   [-1, 512, 12, 16]         --\n",
              "|    |    └─Conv2d: 3-97                 [-1, 512, 12, 16]         (2,359,296)\n",
              "|    |    └─BatchNorm2d: 3-98            [-1, 512, 12, 16]         (1,024)\n",
              "|    |    └─ReLU: 3-99                   [-1, 512, 12, 16]         --\n",
              "├─AdaptiveAvgPool2d: 1-9                 [-1, 512, 1, 1]           --\n",
              "├─Linear: 1-10                           [-1, 6]                   3,078\n",
              "==========================================================================================\n",
              "Total params: 21,287,750\n",
              "Trainable params: 3,078\n",
              "Non-trainable params: 21,284,672\n",
              "Total mult-adds (G): 14.40\n",
              "==========================================================================================\n",
              "Input size (MB): 2.25\n",
              "Forward/backward pass size (MB): 223.50\n",
              "Params size (MB): 81.21\n",
              "Estimated Total Size (MB): 306.96\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = model.to(device)\n",
        "summary(model,(3,H,W))\n",
        "# 512 x 384 x 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zQtZ4lPVVJ-d"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "expected scalar type Long but found Float",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[90], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# forward + backward + optimize\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m# print(inputs[0]) \u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(inputs\u001b[39m.\u001b[39mdata))\n\u001b[0;32m---> 16\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     18\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     19\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/cos429/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/cos429/lib/python3.9/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/cos429/lib/python3.9/site-packages/torchvision/models/resnet.py:268\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_impl\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    267\u001b[0m     \u001b[39m# See note [TorchScript super()]\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[1;32m    269\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(x)\n\u001b[1;32m    270\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/cos429/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/cos429/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/cos429/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"
          ]
        }
      ],
      "source": [
        "for epoch in range(1):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    exp_lr_scheduler.step()\n",
        "    model.train()\n",
        "    for inputs, labels in dataloaders['train']:\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer_ft.zero_grad()\n",
        "        # forward + backward + optimize\n",
        "        # print(inputs[0]) \n",
        "        print(type(inputs.data))\n",
        "        outputs = model(inputs)\n",
        "        \n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_ft.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/shiho/opt/anaconda3/envs/cos429/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "expected scalar type Long but found Float",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m optimizer_ft\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     11\u001b[0m \u001b[39m# forward + backward + optimize\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     13\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     14\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/cos429/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/cos429/lib/python3.9/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/cos429/lib/python3.9/site-packages/torchvision/models/resnet.py:268\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_impl\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    267\u001b[0m     \u001b[39m# See note [TorchScript super()]\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[1;32m    269\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(x)\n\u001b[1;32m    270\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/cos429/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/cos429/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "File \u001b[0;32m~/opt/anaconda3/envs/cos429/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
            "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"
          ]
        }
      ],
      "source": [
        "# for epoch in range(1):  # loop over the dataset multiple times\n",
        "\n",
        "#     running_loss = 0.0\n",
        "#     exp_lr_scheduler.step()\n",
        "#     for i, data in enumerate(dataloaders['train'], 0):\n",
        "#         # get the inputs; data is a list of [inputs, labels]\n",
        "#         inputs, labels = data\n",
        "\n",
        "#         # zero the parameter gradients\n",
        "#         optimizer_ft.zero_grad()\n",
        "#         # forward + backward + optimize\n",
        "#         outputs = model(inputs)\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer_ft.step()\n",
        "\n",
        "#         # print statistics\n",
        "#         running_loss += loss.item()\n",
        "#         if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "#             print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "#             running_loss = 0.0\n",
        "\n",
        "# print('Finished Training')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## create dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4zd7P8VDIgJ",
        "outputId": "31c731c0-e561-4d08-f982-0b4b811f29bd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([39.65176098, 47.01226751, 31.89552829, 38.14800158, 32.44954491,\n",
              "       10.84289672])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "arr = np.array([501,594,403,482,410,137])\n",
        "200*arr/np.sum(arr)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
